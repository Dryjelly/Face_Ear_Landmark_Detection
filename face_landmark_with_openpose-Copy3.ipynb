{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# automatically reload edited modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "from scipy import io"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "![alt text](https://ibug.doc.ic.ac.uk/media/uploads/images/ear-examplar.png \"ears_data_set_image\")\n",
    "\n",
    "출처 : https://ibug.doc.ic.ac.uk/resources/ibug-ears/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data_dir = '../DATA_SET/Ear_data/train/o_images/'\n",
    "# landmark_dir = '../DATA_SET/Ear_data/train/o_landmarks/'\n",
    "\n",
    "data_dir = '../DATA_SET/300W-LP/300W_LP/AFW/'\n",
    "landmark_dir = '../DATA_SET/300W-LP/300W_LP/landmarks/AFW/'\n",
    "\n",
    "file_list = glob.glob(data_dir+'*_0.jpg')\n",
    "file_list = map(os.path.basename, file_list)\n",
    "# file_list = os.listdir(data_dir)\n",
    "name_list = list(set(map(lambda x : x[:-4], file_list)))\n",
    "\n",
    "def _read_mat(file_path):\n",
    "    matfile = io.loadmat(file_path)\n",
    "    label = (matfile['pts_2d']).astype(np.float32)\n",
    "    return label\n",
    "\n",
    "\"\"\"\n",
    "for name in name_list[:1]:\n",
    "    image = im.imread(os.path.join(data_dir, name+'.png'))\n",
    "    print(image.shape)\n",
    "    with open(landmark_dir+name+'.txt', 'r') as f:\n",
    "        lines_list = f.readlines()\n",
    "        temp = list(map(lambda l: list(map(float, l.split(' '))), lines_list[3:-1]))\n",
    "        print(np.array(temp).shape)\n",
    "        for line in lines_list[3:-1]:\n",
    "            x, y = list(map(float,line.split(' ')))\n",
    "            image = cv2.circle(image, (int(x), int(y)), 3, (1, 0, 0), -1)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "for name in name_list[:1]:\n",
    "    image = im.imread(os.path.join(data_dir, name+'.jpg'))\n",
    "    print(image.shape)\n",
    "\n",
    "    label = _read_mat(landmark_dir+name+'_pts.mat')\n",
    "    for x, y in label:\n",
    "        cv2.circle(image, (int(x), int(y)), 3, (255, 0, 0), -1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# image resize \n",
    "for name in name_list[:1]:\n",
    "    image = im.imread(os.path.join(data_dir, name+'.jpg'))\n",
    "    print(image.shape)\n",
    "\n",
    "    resized_image = tf.image.resize(image ,[int(image.shape[0]/4), int(image.shape[0]/2)])\n",
    "\n",
    "    label = _read_mat(landmark_dir+name+'_pts.mat')\n",
    "    for x, y in label:\n",
    "        cv2.circle(image, (int(x), int(y)), 3, (255, 0, 0), -1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    idx = np.array([image.shape[1]/2, image.shape[0]/2])\n",
    "    norm_label = (label-idx)/idx\n",
    "    idx = np.array([resized_image.shape[1]/2, resized_image.shape[0]/2])\n",
    "    new_label = (norm_label*idx)+idx+int(image.shape[0]/4)\n",
    "\n",
    "    str = int(image.shape[0]/4)\n",
    "    dst = str + resized_image.shape[0]\n",
    "    pad_image = np.zeros_like(image)\n",
    "    pad_image[str:dst,str:dst,:] = resized_image\n",
    "    \n",
    "    for x, y in new_label:\n",
    "        cv2.circle(pad_image, (int(x), int(y)), 3, (255, 0, 0), -1)\n",
    "\n",
    "    plt.imshow(pad_image)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image.shape[0]/2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "landmark = []\n",
    "for line in lines_list[3:-1]:\n",
    "    x, y = list(map(float,line.split(' ')))\n",
    "    landmark.append([int(x), int(y)])\n",
    "landmark = np.array(landmark)\n",
    "min_x = np.min(landmark[:,0])\n",
    "max_x = np.max(landmark[:,0])\n",
    "min_y = np.min(landmark[:,1])\n",
    "max_y = np.max(landmark[:,1])\n",
    "print(min_x, max_x, min_y, max_y)\n",
    "\n",
    "image_left = image[:,0:min_x,:]\n",
    "image_right = image[:,max_x:-1,:]\n",
    "image_mid = image[:,min_x:max_x:3,:]\n",
    "\n",
    "new_image = np.concatenate((image_left, image_mid, image_right), axis = 1)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(new_image)\n",
    "plt.show()\n",
    "\n",
    "landmark = (landmark-np.array([min_x,0]))/np.array([3,1])+np.array([min_x,0])\n",
    "for x,y in landmark:\n",
    "    new_image = cv2.circle(new_image, (int(x), int(y)), 3, (0, 1, 0), -1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 설정 및 파라미터 설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "INPUT_SIZE = 200\n",
    "MAP_SIZE = 100\n",
    "MAP_SIGMA = 2.5\n",
    "\n",
    "LANDMARK_NUM = list(range(68))#[4,9,16,36] # 원하는 point 를 입력\n",
    "LANDMARK_SIZE = len(LANDMARK_NUM) # ear : 55 / face : 68\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 200\n",
    "\n",
    "split_rate = 0.9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#-------------------------------------- model setting\n",
    "#------------------------- feature map\n",
    "x = tf.keras.Input(shape=(INPUT_SIZE, INPUT_SIZE, 3), name='input_layer')\n",
    "h = tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding = 'same')(x)\n",
    "h_1 = tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.MaxPool2D()(h_1)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h_2 = tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.MaxPool2D()(h_2)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h_3 = tf.keras.layers.Conv2D(256, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.MaxPool2D()(h_3)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(3,3), activation='relu', padding = 'same')(h)\n",
    "feature_map = tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu', padding = 'same', name='feature_map')(h)\n",
    "\n",
    "#------------------------- stage1\n",
    "h = tf.keras.layers.Conv2D(512, kernel_size=(1,1), activation='relu', name='stage_1')(feature_map)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s1 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s1')(h)\n",
    "\n",
    "s1_h = tf.keras.layers.MaxPool2D((4,4))(s1)\n",
    "\n",
    "#------------------------- stage2\n",
    "h = tf.keras.layers.concatenate([feature_map, s1_h], axis=-1, name='stage_2')\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s2 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s2')(h)\n",
    "\n",
    "s2_h = tf.keras.layers.MaxPool2D((4,4))(s2)\n",
    "\n",
    "#------------------------- stage3\n",
    "h = tf.keras.layers.concatenate([feature_map, s2_h], axis=-1, name='stage_3')\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s3 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s3')(h)\n",
    "\n",
    "s3_h = tf.keras.layers.MaxPool2D((4,4))(s3)\n",
    "\n",
    "#------------------------- stage4\n",
    "h = tf.keras.layers.concatenate([feature_map, s3_h], axis=-1, name='stage_4')\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s4 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s4')(h)\n",
    "\n",
    "s4_h = tf.keras.layers.MaxPool2D((4,4))(s4)\n",
    "\n",
    "#------------------------- stage5\n",
    "h = tf.keras.layers.concatenate([feature_map, s4_h], axis=-1, name='stage_5')\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s5 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s5')(h)\n",
    "\n",
    "s5_h = tf.keras.layers.MaxPool2D((4,4))(s5)\n",
    "\n",
    "#------------------------- stage6\n",
    "h = tf.keras.layers.concatenate([feature_map, s5_h], axis=-1, name='stage_6')\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(7,7), activation='relu', padding = 'same')(h)\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "\n",
    "h = tf.keras.layers.Conv2D(256, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_3])\n",
    "\n",
    "h = tf.keras.layers.Conv2D(128, kernel_size=(1,1), activation='relu')(h)\n",
    "h = tf.keras.layers.UpSampling2D()(h)\n",
    "h = tf.keras.layers.Add()([h, h_2])\n",
    "\n",
    "s6 = tf.keras.layers.Conv2D(LANDMARK_SIZE, kernel_size=(1,1), name='s6')(h)\n",
    "\n",
    "#------------------------- return\n",
    "output = h = tf.keras.layers.concatenate([s1, s2, s3, s4, s5, s6], axis=-1, name='output_layer')\n",
    "#-------------------------------------- model setting end\n",
    "\n",
    "def _stage_loss(y_true, y_pred):\n",
    "    stage = 6\n",
    "    #y_ture = tf.image.resize(y_true, size=[feat_size, feat_size])\n",
    "    #threshold = 0.0001\n",
    "    mask = y_true != 0 #> threshold\n",
    "    resized_mask   = tf.tile(mask, [1,1,1,stage])\n",
    "    resized_y_true = tf.tile(y_true, [1,1,1,stage])\n",
    "    resized_y_pred = y_pred\n",
    "    #resized_y_pred = tf.image.resize(y_pred, size=[INPUT_SIZE, INPUT_SIZE])\n",
    "    resized_y_pred = tf.image.resize(y_pred, size=[MAP_SIZE, MAP_SIZE])\n",
    "    \n",
    "    loss = tf.math.reduce_mean(tf.math.square(resized_y_true - resized_y_pred) * tf.cast(resized_mask, tf.float32), axis=-1)\n",
    "    #loss = tf.math.reduce_mean(tf.math.square(resized_y_true - resized_y_pred), axis=-1)\n",
    "    \n",
    "    #loss = tf.keras.losses.mean_squared_error(resized_y_true, resized_y_pred)\n",
    "    return loss\n",
    "\n",
    "model = tf.keras.Model(inputs=[x], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=_stage_loss)\n",
    "\n",
    "# model = keras.models.load_model('saved_model_openpose_ears_ver4', compile=False)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=_stage_loss)\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def _stage_loss(y_true, y_pred):\n",
    "    stage = 6\n",
    "    #y_ture = tf.image.resize(y_true, size=[feat_size, feat_size])\n",
    "    #threshold = 0.0001\n",
    "    mask = y_true != 0 #> threshold\n",
    "    resized_mask   = tf.tile(mask, [1,1,1,stage])\n",
    "    resized_y_true = tf.tile(y_true, [1,1,1,stage])\n",
    "    resized_y_pred = y_pred\n",
    "    #resized_y_pred = tf.image.resize(y_pred, size=[INPUT_SIZE, INPUT_SIZE])\n",
    "    resized_y_pred = tf.image.resize(y_pred, size=[MAP_SIZE, MAP_SIZE])\n",
    "    \n",
    "    loss = tf.math.reduce_mean(tf.math.square(resized_y_true - resized_y_pred) * tf.cast(resized_mask, tf.float32), axis=-1)\n",
    "    #loss = tf.math.reduce_mean(tf.math.square(resized_y_true - resized_y_pred), axis=-1)\n",
    "    \n",
    "    #loss = tf.keras.losses.mean_squared_error(resized_y_true, resized_y_pred)\n",
    "    return loss\n",
    "model = keras.models.load_model('saved_model_openpose_face', compile=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=_stage_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터 전처리 설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_size_h = INPUT_SIZE/2\n",
    "feat_size = model.get_layer('feature_map').output_shape[1]\n",
    "\n",
    "#--------------------------------------\n",
    "\n",
    "train_len = int(len(name_list)*split_rate)\n",
    "test_len = len(name_list)-train_len\n",
    "print(train_len, test_len)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(name_list[:train_len])\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(name_list[train_len:])\n",
    "\n",
    "def process_path(name):\n",
    "    image_path = data_dir+name+'.jpg'\n",
    "    #image_path = data_dir+name+'.png'\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    #image = tf.image.decode_png(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.cond(tf.shape(image)[-1] != 3,\n",
    "                    lambda: tf.image.grayscale_to_rgb(image),\n",
    "                    lambda: tf.identity(image))\n",
    "    image_shape = tf.shape(image)\n",
    "    \n",
    "    label = tf.numpy_function(_read_mat, [landmark_dir+name+'_pts.mat', image_shape], tf.float32)\n",
    "    #label = tf.numpy_function(_read_txt, [landmark_dir+name+'.txt', image_shape], tf.float32)\n",
    "    \n",
    "    #image, label = tf.numpy_function(_shrink_image_one, [image, label], [tf.float32, tf.float32])\n",
    "    #image, label = tf.numpy_function(_crop_image_one, [image, label], [tf.float32, tf.float32])  \n",
    "    image, label = tf.numpy_function(_downsize_image, [image, label], [tf.float32, tf.float32]) \n",
    "    image, label = tf.numpy_function(_shift_image_one, [image, label], [tf.float32, tf.float32])\n",
    "    image, label = tf.py_function(_flip_image_one, [image, label], [tf.float32, tf.float32])\n",
    "    \n",
    "    label = tf.numpy_function(_make_confidence_map, [label, MAP_SIGMA], tf.float32)\n",
    "    \n",
    "    image.set_shape([None, None, None])\n",
    "    image = tf.image.resize(image, [INPUT_SIZE, INPUT_SIZE])\n",
    "    \n",
    "    #image.set_shape([INPUT_SIZE, INPUT_SIZE, 3])\n",
    "    label.set_shape([MAP_SIZE, MAP_SIZE, LANDMARK_SIZE])\n",
    "    #image = tf.reshape(image, shape=[input_size, input_size, 3])\n",
    "    #label = tf.reshape(label, shape=[-1])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def _read_mat(file_path, input_shape):\n",
    "    matfile = io.loadmat(file_path)\n",
    "    norm = [input_shape[1]/2, input_shape[0]/2]\n",
    "    label = ((matfile['pts_2d']-norm)/norm).astype(np.float32)\n",
    "    return label\n",
    "\n",
    "def _read_txt(file_path, input_shape):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines_list = f.readlines()\n",
    "        temp = list(map(lambda l : list(map(float, l.split(' '))), lines_list[3:-1]))\n",
    "        temp = np.array(temp)\n",
    "    norm = [input_shape[1]/2, input_shape[0]/2]\n",
    "    label = ((temp-norm)/norm).astype(np.float32)\n",
    "    return label[LANDMARK_NUM]\n",
    "\n",
    "def _make_confidence_map(label, sigma = 2.5):\n",
    "    \n",
    "    norm = [MAP_SIZE/2, MAP_SIZE/2]\n",
    "    new_label = label*norm+norm\n",
    "    \n",
    "    grid_x = np.tile(np.arange(MAP_SIZE), (MAP_SIZE, 1))\n",
    "    grid_y = np.tile(np.arange(MAP_SIZE), (MAP_SIZE, 1)).transpose()\n",
    "    grid_x = np.tile(np.expand_dims(grid_x, axis=-1),LANDMARK_SIZE)\n",
    "    grid_y = np.tile(np.expand_dims(grid_y, axis=-1),LANDMARK_SIZE)\n",
    "    \n",
    "    grid_distance = (grid_x - new_label[:,0]) ** 2 + (grid_y - new_label[:,1]) ** 2\n",
    "    confidence_map = np.exp(-1 * grid_distance / sigma ** 2) # why 0.5?\n",
    "    \n",
    "    return confidence_map.astype(np.float32)\n",
    "\n",
    "def _crop_image_one(img, label): # with label norm\n",
    "    pad = 1\n",
    "    \n",
    "    img_h, img_w, img_c = img.shape\n",
    "    \n",
    "    idx = np.array([img_w/2, img_h/2])\n",
    "    \n",
    "    label = label*idx+idx\n",
    "    \n",
    "    label_x_info = np.array([min(label[:,0]), max(label[:,0])])\n",
    "    label_y_info = np.array([min(label[:,1]), max(label[:,1])])\n",
    "    \n",
    "    ear_w = label_x_info[1]-label_x_info[0]\n",
    "    ear_h = label_y_info[1]-label_y_info[0]\n",
    "    \n",
    "#     s_x = max(int(label_x_info[0]-ear_w*1-pad), 0)\n",
    "#     e_x = min(int(label_x_info[0]+ear_w*8), img_w)\n",
    "#     s_y = max(int(label_y_info[0]-ear_h*2-pad), 0)\n",
    "#     e_y = min(int(label_y_info[0]+ear_h*3), img_h)\n",
    "\n",
    "    s_x = max(int(label_x_info[0]-ear_w*10), 0)\n",
    "    e_x = min(int(label_x_info[0]+ear_w*10), img_w)\n",
    "    s_y = max(int(label_y_info[0]-ear_h*10), 0)\n",
    "    e_y = min(int(label_y_info[0]+ear_h*10), img_h)\n",
    "\n",
    "#     s_x = max(int(label_x_info[0]-ear_w*1-pad), 0)\n",
    "#     e_x = min(int(label_x_info[1]+ear_w*1), img_w)\n",
    "#     s_y = max(int(label_y_info[0]-ear_h*1-pad), 0)\n",
    "#     e_y = min(int(label_y_info[1]+ear_h*1), img_h)\n",
    "    \n",
    "#     s_x = max(int(label_x_info[0]-pad), 0)\n",
    "#     e_x = min(int(label_x_info[1]+pad), img_w)\n",
    "#     s_y = max(int(label_y_info[0]-pad), 0)\n",
    "#     e_y = min(int(label_y_info[1]+pad), img_h)\n",
    "\n",
    "    c_img = img[s_y:e_y, s_x:e_x, :]\n",
    "    c_label = label - np.array([s_x, s_y])\n",
    "    \n",
    "    new_img_h, new_img_w, _ = c_img.shape\n",
    "    \n",
    "    norm = [new_img_w/2, new_img_h/2]\n",
    "    c_label = ((c_label-norm)/norm).astype(np.float32)\n",
    "    return c_img, c_label\n",
    "\n",
    "def _shrink_image_one(img, label):\n",
    "    img_h, img_w, img_c = img.shape\n",
    "    idx = np.array([img_w/2, img_h/2])\n",
    "    label = label*idx+idx\n",
    "    \n",
    "    max_ratio = 4\n",
    "    sh_ratio = np.random.randint(1,max_ratio)\n",
    "    \n",
    "    min_x = int(np.min(label[:,0]))\n",
    "    max_x = int(np.max(label[:,0]))\n",
    "\n",
    "    image_left = img[:,0:min_x,:]\n",
    "    image_right = img[:,max_x:-1,:]\n",
    "    image_mid = img[:,min_x:max_x:sh_ratio,:]\n",
    "\n",
    "    sh_img = np.concatenate((image_left, image_mid, image_right), axis = 1)\n",
    "    sh_label = (label-np.array([min_x,0]))/np.array([sh_ratio,1])+np.array([min_x,0])\n",
    "    \n",
    "    new_img_h, new_img_w, _ = sh_img.shape\n",
    "    norm = [new_img_w/2, new_img_h/2]\n",
    "    sh_label = ((sh_label-norm)/norm).astype(np.float32)\n",
    "    \n",
    "    return sh_img, sh_label\n",
    "    \n",
    "def _flip_image_one(img, label):\n",
    "    c = np.random.randint(2)\n",
    "    f_img, f_label = tf.cond(c==1,\n",
    "                             lambda: (tf.image.flip_left_right(img), label * np.array([-1, 1])),\n",
    "                             lambda: (img, label))\n",
    "    #f_img = tf.image.resize(f_img, [input_size, input_size])\n",
    "    return f_img, f_label\n",
    "\n",
    "def _shift_image_one(img, label, padding = b'zero'):\n",
    "    img_h, img_w, img_c = img.shape\n",
    "    label_p, _ = label.shape\n",
    "    \n",
    "    if padding == b'ori': s_img = img[:]\n",
    "    elif padding == b'zero': s_img = np.zeros_like(img, dtype=np.float32)\n",
    "    s_label = np.expand_dims(label, axis=0)\n",
    "    \n",
    "    label_x_info = np.array([min(label[:,0]), max(label[:,0])])* img_w/2 + img_w/2\n",
    "    label_y_info = np.array([min(label[:,1]), max(label[:,1])])* img_h/2 + img_h/2\n",
    "    \n",
    "    label_x_info = label_x_info.astype(np.int)\n",
    "    label_y_info = label_y_info.astype(np.int)\n",
    "    \n",
    "    shift_x = np.random.randint(-label_x_info[0], img_w - label_x_info[1])\n",
    "    shift_y = np.random.randint(-label_y_info[0], img_h - label_y_info[1])\n",
    "    \n",
    "    shift_x = min(shift_x, img_w//6)\n",
    "    shift_y = min(shift_y, img_h//6)\n",
    "    \n",
    "    if shift_x < 0:\n",
    "        get_x = (-shift_x, img_w)\n",
    "        put_x = (0, img_w + shift_x)\n",
    "    else:\n",
    "        get_x = (0, img_w - shift_x)\n",
    "        put_x = (shift_x, img_w)\n",
    "    if shift_y < 0:\n",
    "        get_y = (-shift_y, img_h)\n",
    "        put_y = (0, img_h + shift_y)\n",
    "    else:\n",
    "        get_y = (0, img_h - shift_y)\n",
    "        put_y = (shift_y, img_h)\n",
    "\n",
    "    if padding == b'edge': s_img = np.pad(img[get_y[0]:get_y[1], get_x[0]:get_x[1], :], ((img_h-get_y[1],get_y[0]),(img_w-get_x[1],get_x[0]),(0,0)), mode='edge')\n",
    "    else: s_img[put_y[0]:put_y[1], put_x[0]:put_x[1], :] = img[get_y[0]:get_y[1], get_x[0]:get_x[1], :]\n",
    "    s_label = np.append(np.expand_dims(label[:,0] + (shift_x)/(img_w/2), axis = -1),\n",
    "                          np.expand_dims(label[:,1] + (shift_y)/(img_h/2), axis = -1), axis = 1)\n",
    "    \n",
    "    return s_img, s_label\n",
    "\n",
    "def _downsize_image(img, label):\n",
    "    img_h, img_w, img_c = img.shape\n",
    "\n",
    "    size = 1 + np.random.randint(101)/100 # 1~2 (100 step)\n",
    "    resized_image = cv2.resize(img ,(int(img_w/size), int(img_h/size)))\n",
    "    #resized_image = tf.image.resize(image ,[int(img_w/size), int(img_h/size)])\n",
    "\n",
    "    new_img_h, new_img_w, _ = resized_image.shape\n",
    "\n",
    "    str = np.array([int(img_w/2 * (1 - 1/size)), int(img_h/2 * (1 - 1/size))])\n",
    "    dst = str + np.array([new_img_w, new_img_h])\n",
    "\n",
    "    idx = np.array([new_img_w/2, new_img_h/2])\n",
    "    new_label = (label*idx)+idx + str\n",
    "\n",
    "    pad_image = np.zeros_like(img, dtype=np.float32)\n",
    "    pad_image[str[1]:dst[1],str[0]:dst[0],:] = resized_image\n",
    "\n",
    "    norm = [img_w/2, img_h/2]\n",
    "    new_label = ((new_label-norm)/norm).astype(np.float32)\n",
    "\n",
    "    return pad_image, new_label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.repeat()\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "for image_raw, label_raw in train_dataset.take(1):\n",
    "    images = image_raw.numpy()\n",
    "    labels = label_raw.numpy()\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    \n",
    "#     norm = np.array([INPUT_SIZE/2, INPUT_SIZE/2])\n",
    "#     labels = labels.reshape(BATCH_SIZE, -1, 2)\n",
    "#     labels = labels*norm + norm\n",
    "    \n",
    "    for i in range(len(images[:5])):\n",
    "        \n",
    "#         for x, y in labels[i]:\n",
    "#             image_o = cv2.circle(images[i], (round(x), round(y)), 1, (1, 0, 0), -1)\n",
    "        \n",
    "        \n",
    "        heatmap = tf.image.resize(labels[i], (INPUT_SIZE, INPUT_SIZE))\n",
    "        #mask = np.amax(heatmap, axis=-1, keepdims=True) > 0.1\n",
    "        mask = heatmap != 0#> 0.0001\n",
    "        mask = np.amax(mask, axis=-1, keepdims=True)\n",
    "        print(mask.shape)\n",
    "        heatmap = cv2.applyColorMap(np.uint8(np.tile(np.amax(heatmap, axis=-1, keepdims=True), 3)*255), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        image_o = images[i]*0.5 + heatmap/255*0.5\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image_o)#*(mask+0.5)/1.5)\n",
    "        print(image_o)\n",
    "        plt.show()\n",
    "        #print(repr(images[i][:100]))\n",
    "        #print()\n",
    "        #print(label_text[i].numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 학습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_step = train_len//BATCH_SIZE\n",
    "test_step = test_len//BATCH_SIZE\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=EPOCH,\n",
    "                    steps_per_epoch=train_step,\n",
    "                    validation_steps=test_step,\n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = tf.keras.models.load_model('saved_model_openpose_face_a2a.h5', compile=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 결과 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ear_hole = np.where(np.array(LANDMARK_NUM) == 36)[0]\n",
    "ear_edge = np.where(np.array(LANDMARK_NUM) == 9)[0]\n",
    "\n",
    "pred = tf.keras.backend.function([model.input], [model.get_layer('s6').output])\n",
    "for image_raw, ture_label in test_dataset.take(3):\n",
    "    images = image_raw.numpy()\n",
    "    #labels = model.predict(images) \n",
    "    labels = pred([images])[0]\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        heatmap = tf.image.resize(labels[i], (INPUT_SIZE, INPUT_SIZE))\n",
    "        temp = np.argmax(np.reshape(heatmap, (-1,LANDMARK_SIZE)), axis = 0)\n",
    "        heatmap = cv2.applyColorMap(np.uint8(np.tile(np.amax(heatmap, axis=-1, keepdims=True), 3)*255), cv2.COLORMAP_JET)\n",
    "        #heatmap = cv2.applyColorMap(np.uint8(np.tile(np.expand_dims(heatmap[:,:,-1], axis = -1), 3)*255), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        image_o = images[i]*0.5 + heatmap/255*0.5\n",
    "        image_ear = images[i][:]*1\n",
    "        image_hole = images[i][:]*1\n",
    "        #plt.imshow(image_o)\n",
    "        #plt.show()\n",
    "        \n",
    "        #temp = np.amax(labels[i], axis=-1)\n",
    "        #temp = np.argmax(labels[i].reshape(-1,LANDMARK_SIZE), axis = 0)\n",
    "        #line = [[],[]]\n",
    "        t_label = tf.image.resize(ture_label[i], (INPUT_SIZE, INPUT_SIZE))\n",
    "        t_label = np.argmax(np.reshape(t_label, (-1,LANDMARK_SIZE)), axis = 0)\n",
    "        min_x, max_x = INPUT_SIZE, 0\n",
    "        min_y, max_y = INPUT_SIZE, 0\n",
    "        mg = 5\n",
    "        hole = []\n",
    "        \n",
    "        for num, idx in enumerate(temp):\n",
    "            #x, y = (idx%feat_size)/feat_size, (idx//feat_size)/feat_size\n",
    "            x, y = idx%INPUT_SIZE, idx//INPUT_SIZE\n",
    "            t_x, t_y = t_label[num]%INPUT_SIZE, t_label[num]//INPUT_SIZE\n",
    "            #x *= INPUT_SIZE\n",
    "            #y *= INPUT_SIZE\n",
    "            \n",
    "            min_x, max_x = max(min(int(t_x)-mg, min_x), 0), min(max(int(t_x)+mg, max_x), INPUT_SIZE)\n",
    "            min_y, max_y = max(min(int(t_y)-mg, min_y), 0), min(max(int(t_y)+mg, max_y), INPUT_SIZE)\n",
    "            if num == ear_edge or num == ear_hole: hole.append([x, y])\n",
    "            if num == ear_hole: image_ear = cv2.circle(image_ear, (round(x), round(y)), 1, (0, 1, 0), -1)\n",
    "            else:        image_ear = cv2.circle(image_ear, (round(x), round(y)), 1, (1, 0, 0), -1)\n",
    "            image_ear = cv2.circle(image_ear, (int(t_x), int(t_y)), 2, (0, 0, 1), 1)\n",
    "            image_o = cv2.circle(image_o, (round(x), round(y)), 1, (1, 0, 0), -1)\n",
    "            #line[0].append(x), line[1].append(y)\n",
    "        #line[0].append(line[0][0]), line[1].append(line[1][0])\n",
    "        #np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
    "        #print(temp.shape, temp)\n",
    "        #print(loc)\n",
    "        h_x, h_y = hole[0][0] + (hole[1][0] - hole[0][0])*0.8, hole[0][1] + (hole[1][1] - hole[0][1])*0.8\n",
    "        image_hole = cv2.circle(image_hole, (int(h_x), int(h_y)), 1, (0, 0, 1), -1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(image_o)\n",
    "        #plt.plot(line[0], line[1], '--', linewidth=1, color='firebrick')\n",
    "        #plt.show()\n",
    "        #print(min_x, max_x, min_y, max_y)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(image_ear[min_y:max_y, min_x:max_x,:])\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(image_hole[min_y:max_y, min_x:max_x,:])\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('saved_model_openpose_face')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = tf.keras.backend.function([model.input], [model.get_layer('s6').output])\n",
    "for image_raw, ture_label in train_dataset.take(3):\n",
    "    images = image_raw.numpy()\n",
    "    #labels = model.predict(images) \n",
    "    labels = pred([images])[0]\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        heatmap = tf.image.resize(labels[i], (INPUT_SIZE, INPUT_SIZE))\n",
    "        temp = np.argmax(np.reshape(heatmap, (-1,LANDMARK_SIZE)), axis = 0)\n",
    "        heatmap = cv2.applyColorMap(np.uint8(np.tile(np.amax(heatmap, axis=-1, keepdims=True), 3)*255), cv2.COLORMAP_JET)\n",
    "        #heatmap = cv2.applyColorMap(np.uint8(np.tile(np.expand_dims(heatmap[:,:,-1], axis = -1), 3)*255), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        image_o = images[i]*0.5 + heatmap/255*0.5\n",
    "        \n",
    "        for num, idx in enumerate(temp):\n",
    "            #x, y = (idx%feat_size)/feat_size, (idx//feat_size)/feat_size\n",
    "            x, y = idx%INPUT_SIZE, idx//INPUT_SIZE\n",
    "            t_x, t_y = t_label[num]%INPUT_SIZE, t_label[num]//INPUT_SIZE\n",
    "            #x *= INPUT_SIZE\n",
    "            #y *= INPUT_SIZE\n",
    "            \n",
    "            min_x, max_x = max(min(int(t_x)-mg, min_x), 0), min(max(int(t_x)+mg, max_x), INPUT_SIZE)\n",
    "            min_y, max_y = max(min(int(t_y)-mg, min_y), 0), min(max(int(t_y)+mg, max_y), INPUT_SIZE)\n",
    "            if num == ear_edge or num == ear_hole: hole.append([x, y])\n",
    "            if num == ear_hole: image_ear = cv2.circle(image_ear, (round(x), round(y)), 1, (0, 1, 0), -1)\n",
    "            else:        image_ear = cv2.circle(image_ear, (round(x), round(y)), 1, (1, 0, 0), -1)\n",
    "            image_ear = cv2.circle(image_ear, (int(t_x), int(t_y)), 2, (0, 0, 1), 1)\n",
    "            image_o = cv2.circle(image_o, (round(x), round(y)), 1, (1, 0, 0), -1)\n",
    "\n",
    "        plt.imshow(image_o)\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "pred = tf.keras.backend.function([model.input], [model.get_layer('s6').output])\n",
    "\n",
    "for image_raw, ture_label in train_dataset.take(3):\n",
    "    images = image_raw.numpy()\n",
    "    heatmaps = pred([images])[0]\n",
    "    for heatmap in heatmaps:\n",
    "        all_peaks = []\n",
    "        peak_counter = 0\n",
    "        heatmap = heatmap[:,:,0]\n",
    "\n",
    "        heatmap = gaussian_filter(heatmap, sigma=2.5)\n",
    "        map_left = np.zeros(heatmap.shape)\n",
    "        map_right = np.zeros(heatmap.shape)\n",
    "        map_top = np.zeros(heatmap.shape)\n",
    "        map_bottom = np.zeros(heatmap.shape)\n",
    "        map_left[1:, :] = heatmap[:-1, :]\n",
    "        map_right[:-1, :] = heatmap[1:, :]\n",
    "        map_top[:, 1:] = heatmap[:, :-1]\n",
    "        map_bottom[:, :-1] = heatmap[:, 1:]\n",
    "\n",
    "        peaks_binary = np.logical_and.reduce((\n",
    "            heatmap > 0.05,\n",
    "            heatmap > map_left,\n",
    "            heatmap > map_right,\n",
    "            heatmap > map_top,\n",
    "            heatmap > map_bottom,\n",
    "        ))\n",
    "\n",
    "        peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])  # [(x, y), (x, y)...]\n",
    "        all_peaks = xp.array([peak for peaks_each_category in all_peaks for peak in peaks_each_category])\n",
    "\n",
    "        plt.imshow(heatmap)\n",
    "        plt.show()\n",
    "        plt.imshow(peaks_binary)\n",
    "        plt.show()\n",
    "        print(\"-------------------------\")\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "interpreter": {
   "hash": "2548794439710c6fbdf3c5c0aaf270804fe885e201ec4a97e54578b1b1eb17aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}